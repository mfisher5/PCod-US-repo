{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacks batch 2 - notebook 3\n",
    "\n",
    "\n",
    "This notebook contains all of the extra filtering steps (including HWE). When specifying filtering for MAF, missing data, I did not include the Pribilof Islands populations because that DNA was of poor quality and most individuals were missing >50% of genotypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/PCod-US-repo/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering\n"
     ]
    }
   ],
   "source": [
    "cd PostStacksFiltering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python genBASH_finalfilters.py Input_extraFilters_b2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x PostStacksFilters_batch2_5-26.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<br>\n",
    "I wanted to stop the filtering steps before I hit the MAF filtering because I need to exclude the Pribilof Islands. So I exited the bash shell script and will run the contents of that script manually. Starting with ...\n",
    "\n",
    "\n",
    "<br>\n",
    "**adding \"sample\" title to genepop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding 'sample' as the first column header\r\n"
     ]
    }
   ],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/add_sample_to_genepop.py /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note: in filtering script, switch CorrectedGenos to CorrectedGenotypes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transpose.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/transpose.py /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.CorrectedGenotypes_biallelic_edit.genepop /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.CorrectedGenotypes_biallelic_TRANSPOSED.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note: in filtering script, switch 'transposed.genepop' to 'transposed.txt'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**convert to .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/convert_genepop_to_csv.py /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate MAF filtering script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/PCod-US-repo/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering\n"
     ]
    }
   ],
   "source": [
    "cd PostStacksFiltering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 9 populations.\r\n",
      "These are your populations, with the number of samples in each:\r\n",
      "OrderedDict([('Kodiak03', 48), ('Adak06', 48), ('WashCoast05', 48), ('HecStrait04', 48), ('PribIslands04', 48), ('PugetSound12', 48), ('GeorgiaStrait13', 17), ('PWSound12', 24), ('UnimakPass03', 48)])\r\n"
     ]
    }
   ],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/genMAFfiltering.py /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PopMap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**manually removed PI population from MAF filtering script so that those individuals are written over into the new genepop file, but are not taken into account when looking for MAFs**\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**run MAF filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You filtered out: \r\n",
      "1839 loci\r\n",
      "You retained: \r\n",
      "4538 loci\r\n"
     ]
    }
   ],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/Eleni_filter_by_MinorAlleleFrequency_takeARGS_5-30.py \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.CorrectedGenotypes_biallelic_TRANSPOSED.csv \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.MAFfiltering_outputFreqs \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.filteredMAF \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.MAFfiltering_BADgenotypes \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.MAFfiltering_blacklistedMAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot of loci to filter out for MAF. I decided to first check for individuals with lots of missing data, remove those from the file `batch_2.CorrectedGenotypes_biallelic_TRANSPOSED.csv` and re-run MAF filtering. \n",
    "\n",
    "<br>\n",
    "I first calculated in excel the percent of missing data in each individual. I then created a text file with these values. The code below takes this text file and creates a list of individuals missing 50% or more of their data. It then goes into the PopMap file and removes these individuals, writing out a new file `PopMap_individs_gt50rm.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOD03_087 \t0.650148973\n",
      "KOD03_092 \t0.790026658\n",
      "AD06_002 \t0.668809785\n",
      "AD06_005 \t0.507448644\n",
      "AD06_022 \t0.587109926\n",
      "AD06_028 \t0.511839423\n",
      "AD06_031 \t0.836443469\n",
      "AD06_034 \t0.592598401\n",
      "WC05_038 \t0.796612827\n",
      "WC05_039 \t0.556217657\n",
      "WC05_043 \t0.675082327\n",
      "HS04_011 \t0.670064294\n",
      "HS04_039 \t0.515132507\n",
      "HS04_043 \t0.541477184\n",
      "PI04_001 \t0.925984005\n",
      "PI04_002 \t0.996236475\n",
      "PI04_003 \t0.913438921\n",
      "PI04_004 \t0.992629763\n",
      "PI04_005 \t0.997177356\n",
      "PI04_007 \t0.899482515\n",
      "PI04_008 \t0.874392347\n",
      "PI04_009 \t0.999686373\n",
      "PI04_010 \t0.999372746\n",
      "PI04_011 \t0.920338717\n",
      "PI04_012 \t0.996863729\n",
      "PI04_013 \t0.998745492\n",
      "PI04_014 \t0.99027756\n",
      "PI04_015 \t0.998745492\n",
      "PI04_016 \t0.994354712\n",
      "PI04_018 \t0.996706915\n",
      "PI04_019 \t0.966598714\n",
      "PI04_020 \t0.84224557\n",
      "PI04_021 \t0.996393288\n",
      "PI04_022 \t0.954524071\n",
      "PI04_023 \t0.936176886\n",
      "PI04_024 \t0.998745492\n",
      "PI04_025 \t0.997020543\n",
      "PI04_026 \t0.999059119\n",
      "PI04_027 \t0.993570645\n",
      "PI04_028 \t0.998431865\n",
      "PI04_029 \t0.997647797\n",
      "PI04_030 \t0.852124824\n",
      "PI04_031 \t0.999529559\n",
      "PI04_032 \t0.933667869\n",
      "PI04_033 \t0.998902305\n",
      "PI04_034 \t0.96863729\n",
      "PI04_035 \t0.950603732\n",
      "PI04_037 \t0.99733417\n",
      "PI04_038 \t0.99733417\n",
      "PI04_039 \t0.99780461\n",
      "PI04_040 \t0.94919241\n",
      "PI04_041 \t0.972400815\n",
      "PI04_042 \t0.998902305\n",
      "PI04_043 \t0.998588678\n",
      "PI04_044 \t0.832366316\n",
      "PI04_045 \t0.998902305\n",
      "PI04_046 \t0.981652815\n",
      "PI04_047 \t0.9756939\n",
      "PI04_048 \t0.868903873\n",
      "PS12_043 \t0.675552768\n",
      "GS13_002 \t0.627411008\n",
      "GS13_009 \t0.569546809\n",
      "UP03_026 \t0.782499608\n",
      "UP03_031 \t0.889603262\n",
      "UP03_039 \t0.579112435\n",
      "65  samples removed from Population Map for missing data.\n"
     ]
    }
   ],
   "source": [
    "datafile = open(\"../SampleList_b2_IndividsMissingData.txt\", \"r\")\n",
    "old_samples = open(\"../PopMap.txt\", \"r\")\n",
    "new_samplelist = open(\"../PopMap_individs_gt50rm.txt\", \"w\")\n",
    "\n",
    "samples_to_rm = []\n",
    "missing_count = 0\n",
    "\n",
    "datafile.readline()\n",
    "for line in datafile: \n",
    "    sample = line.strip().split()[1]\n",
    "    p_missing = float(line.strip().split()[0])\n",
    "    if p_missing > float(0.50):\n",
    "        samples_to_rm.append(sample)\n",
    "        print sample, \"\\t\", p_missing\n",
    "        missing_count += 1\n",
    "datafile.close()\n",
    "print missing_count, \" samples removed from Population Map for missing data.\"\n",
    "for line in old_samples:\n",
    "    if line.strip().split()[0] not in samples_to_rm:\n",
    "        new_samplelist.write(line)\n",
    "old_samples.close()\n",
    "new_samplelist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "Now I have to remove those individuals from the transposed genepop file. I wrote a script that will take in a filtered Population Map and the edited text file `batch.CorrectedGenotypes_biallelic_edit.txt`, and then write out a new text file in the same format without the individuals that must be filtered out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Pacific cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have retained  312  individuals\n"
     ]
    }
   ],
   "source": [
    "mycsv = open(\"../../stacks_b2_wgenome/batch_2.CorrectedGenotypes_biallelic_edit.genepop\", \"r\")\n",
    "newcsv = open(\"../../stacks_b2_wgenome/batch_2.CorrectedGenotypes_biallelic_edit_filteredIndivids.genepop\", \"w\")\n",
    "new_samplelist = open(\"../PopMap_individs_gt50rm.txt\", \"r\")\n",
    "\n",
    "retained_individs = []\n",
    "\n",
    "for line in new_samplelist: \n",
    "    retained_individs.append(line.strip().split()[0])\n",
    "new_samplelist.close()\n",
    "print \"You have retained \", len(retained_individs), \" individuals\"\n",
    "\n",
    "loci_line = mycsv.readline()\n",
    "newcsv.write(loci_line)\n",
    "\n",
    "for line in mycsv: \n",
    "    sampleID = line.strip().split()[0]\n",
    "    if sampleID in retained_individs:\n",
    "        newcsv.write(line)\n",
    "mycsv.close()\n",
    "newcsv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** redo transpose.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/transpose.py \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.CorrectedGenotypes_biallelic_edit_filteredIndivids.genepop \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.CorrectedGenotypes_biallelic_TRANSPOSED.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** redo convert_to_csv.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/convert_genepop_to_csv.py \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome \\\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**redo genMAFfiltering.py with new PopMap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 9 populations.\r\n",
      "These are your populations, with the number of samples in each:\r\n",
      "OrderedDict([('Kodiak03', 46), ('Adak06', 42), ('WashCoast05', 45), ('HecStrait04', 45), ('PribIslands04', 3), ('PugetSound12', 47), ('GeorgiaStrait13', 15), ('PWSound12', 24), ('UnimakPass03', 45)])\r\n"
     ]
    }
   ],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/genMAFfiltering.py \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PopMap_individs_gt50rm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit MAF filtering script so that it doesn't take into account Pribilof Islands Individuals, then run MAF filtering script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You filtered out: \r\n",
      "1849 loci\r\n",
      "You retained: \r\n",
      "4528 loci\r\n"
     ]
    }
   ],
   "source": [
    "!python /mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/scripts/PostStacksFiltering/Eleni_filter_by_MinorAlleleFrequency_takeARGS_5-30.py \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.CorrectedGenotypes_biallelic_TRANSPOSED.csv \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.MAFfiltering_outputFreqs \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.filteredMAF \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.MAFfiltering_BADgenotypes \\\n",
    "/mnt/hgfs/Pacific\\ cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome/batch_2.MAFfiltering_blacklistedMAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok that's weird. Not sure why I'm filtering out so many loci at this step..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate script to filter out loci with too much missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 9 populations.\r\n",
      "These are your populations, with the number of samples in each:\r\n",
      "OrderedDict([('Kodiak03', 46), ('Adak06', 42), ('WashCoast05', 45), ('HecStrait04', 45), ('PribIslands04', 3), ('PugetSound12', 47), ('GeorgiaStrait13', 15), ('PWSound12', 24), ('UnimakPass03', 45)])\r\n"
     ]
    }
   ],
   "source": [
    "!python genMissingLoci.py ../PopMap_individs_gt50rm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remove filtering by PribIslands and Georgia Strait**\n",
    "\n",
    "**Run filter loci script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 4528 loci\r\n",
      "Number of loci removed: 1583\r\n"
     ]
    }
   ],
   "source": [
    "!python FilterLoci_by_MissingValues_5-30.py \\\n",
    "../../stacks_b2_wgenome/batch_2.filteredMAF \\\n",
    "../../stacks_b2_wgenome/batch_2.filteredMAF_filteredLoci \\\n",
    "../../stacks_b2_wgenome/batch_2.filteredLoci_blacklisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2945"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4528-1583"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to batch one, which ended up with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2098"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4048-1950"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I filter loci by missing value not including PI, GS AND PWS? \n",
    "\n",
    "**Rerun filter loci script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 4528 loci\r\n",
      "Number of loci removed: 1582\r\n"
     ]
    }
   ],
   "source": [
    "!python FilterLoci_by_MissingValues_5-30_noPIGSPWS.py \\\n",
    "../../stacks_b2_wgenome/batch_2.filteredMAF \\\n",
    "../../stacks_b2_wgenome/batch_2.filteredMAF_filteredLoci_2 \\\n",
    "../../stacks_b2_wgenome/batch_2.filteredLoci_blacklisted_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's very odd that I'm filtering out so many loci at the MAF step, \n",
    "\n",
    "`You filtered out \n",
    "1849 loci`\n",
    "\n",
    "`You retained \n",
    "4528 loci`\n",
    "\n",
    "So to troubleshoot...\n",
    "\n",
    "\n",
    "(1) I first made sure that the MAF script was coded with the correct column numbers for each sample. It is \n",
    "\n",
    "\n",
    "**(2) I'll have to go back into the original fastq files and verify that the reads for that locus really don't have the second allele in that population. **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/PCod-US-repo/stacks_b2_wgenome\n"
     ]
    }
   ],
   "source": [
    "cd ../stacks_b2_wgenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP ONE: Get list of MAF blacklisted loci (loci filtered out for having 2nd allele below MAF cutoff 0.05)\n",
    "blacklist_file = open(\"batch_2.MAFfiltering_blacklistedMAF\", \"r\")\n",
    "blacklisted_loci = []\n",
    "for line in blacklist_file: \n",
    "    if \"Locus\" not in line:\n",
    "        blacklisted_loci.append(line.strip().split()[0])\n",
    "blacklist_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#STEP TWO: Get the allele variants and the position of the SNP in that locus\n",
    "#save in a dictionary with the locus number as the key\n",
    "marines_cat = open(\"batch_2.biallelic_catalog.tsv\", \"r\")\n",
    "\n",
    "import collections\n",
    "snp_dict = collections.defaultdict(list)\n",
    "\n",
    "for line in marines_cat: \n",
    "    if line.strip().split()[0] in blacklisted_loci: \n",
    "        locus = line.strip().split()[0]\n",
    "        linelist = line.strip().split()[1:]\n",
    "        for item in linelist: \n",
    "            snp_dict[locus].append(item)\n",
    "marines_cat.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', 'T', '53']\n"
     ]
    }
   ],
   "source": [
    "print snp_dict['10003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP THREE: write code that will take an individual name + a loci list as an argument, then look up\n",
    "# 1. the corresponding individual locus ID in the .matches file\n",
    "# 2. that locus' reads in the individual's .tags file\n",
    "# 3. the base pair at the given position within each read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9876\n",
      "allele 1 is G\n",
      "allele 2 is T\n",
      "G :  66\n",
      "T :  0\n"
     ]
    }
   ],
   "source": [
    "locus = '10003'\n",
    "sample = 'KOD03_035'\n",
    "\n",
    "#get corresponding individual's locus ID for catalog locus ID\n",
    "matches_file = open(sample+\".matches.tsv\", \"r\")\n",
    "for line in matches_file: \n",
    "    catID= line.strip().split()[2]\n",
    "    if catID == locus:\n",
    "        matchID = line.strip().split()[4]\n",
    "matches_file.close()\n",
    "print matchID\n",
    "\n",
    "#save information in dictionary list as separate arguments: position and alleles 1 and 2\n",
    "snp_info = snp_dict[locus]\n",
    "if len(snp_info) == 3: \n",
    "    allele1 = snp_info[0]\n",
    "    print \"allele 1 is \" + allele1\n",
    "    allele2 = snp_info[1]\n",
    "    print \"allele 2 is \" + allele2\n",
    "    snp_pos = int(snp_info[2])\n",
    "elif len(snp_info) == 4: \n",
    "    print \"There are two snps within this haplotype; you need to work on your code if you want to process this snp\"\n",
    "else: \n",
    "    print \"This is not a biacheck the alleles found at that position in the raw read, using the tags file. llelic SNP... something went wrong in Marine's filtering!\"\n",
    "    \n",
    "    \n",
    "\n",
    "#create a temporary file to hold all of the raw reads at that locus\n",
    "tags_file = open(sample+\".tags.tsv\", \"r\")\n",
    "temp_file = open(sample+\"_reads.txt\", \"w\")\n",
    "\n",
    "tags_file.readline()\n",
    "for line in tags_file: \n",
    "    if not line.startswith(\"#\"):\n",
    "        if line.strip().split(\"\\t\")[2] == matchID and line.strip().split(\"\\t\")[6] == \"primary\": \n",
    "            temp_file.write(line.strip().split(\"\\t\")[9] + \"\\n\")\n",
    "tags_file.close()\n",
    "temp_file.close()\n",
    "\n",
    "#check the alleles found at that position in the raw read, using the temporary file.\n",
    "allele1_count = 0\n",
    "allele2_count = 0\n",
    "temp_file = open(sample+\"_reads.txt\", \"r\")\n",
    "for line in temp_file: \n",
    "    read = list(line.strip())\n",
    "    if read[snp_pos] == allele1: \n",
    "        allele1_count += 1\n",
    "    elif read[snp_pos] == allele2: \n",
    "        allele2_count += 1\n",
    "    else: \n",
    "        print \"neither allele can be found in this read. The base pair at this position is \", read[snp_pos]\n",
    "temp_file.close()\n",
    "    \n",
    "print allele1, \": \", allele1_count\n",
    "print allele2, \": \", allele2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10003', '10030', '10047', '10051', '10053']\n",
      "['KOD03_035', 'KOD03_051', 'KOD03_052', 'KOD03_053', 'KOD03_054', 'KOD03_055', 'KOD03_056', 'KOD03_057', 'KOD03_058']\n"
     ]
    }
   ],
   "source": [
    "## TEST WITH A SMALL LIST OF INDIVIDUALS AND LOCI\n",
    "test_loci = blacklisted_loci[0:5]\n",
    "print test_loci\n",
    "popmap = open(\"../scripts/PopMap_individs_gt50rm.txt\", \"r\")\n",
    "i = 0\n",
    "test_individs = []\n",
    "for line in popmap:\n",
    "    i += 1\n",
    "    if i < 10: \n",
    "        test_individs.append(line.strip().split()[0])\n",
    "    else:\n",
    "        break\n",
    "print test_individs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SNPfinder(locus, sample): \n",
    "    import sys #for exiting function later if something goes wrong\n",
    "    #get corresponding individual's locus ID for catalog locus ID\n",
    "    matches_file = open(sample+\".matches.tsv\", \"r\")\n",
    "    matchID = None\n",
    "    for line in matches_file: \n",
    "        catID= line.strip().split()[2]\n",
    "        if catID == locus:\n",
    "            matchID = line.strip().split()[4]\n",
    "    matches_file.close()\n",
    "    \n",
    "    if matchID is None: \n",
    "        print locus, \" is not present in \", sample\n",
    "        return None\n",
    "    \n",
    "    #save information in dictionary list as separate arguments: position and alleles 1 and 2\n",
    "    snp_info = snp_dict[locus]\n",
    "    if len(snp_info) == 3: \n",
    "        allele1 = snp_info[0]\n",
    "        allele2 = snp_info[1]\n",
    "        snp_pos = int(snp_info[2])\n",
    "    elif len(snp_info) == 4: \n",
    "        print \"There are two snps within this haplotype; you need to work on your code if you want to process this snp\"\n",
    "        return None\n",
    "    else: \n",
    "        print \"This is not a biallelic locus ... something went wrong in Marine's filtering!\"\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    #create a temporary file to hold all of the raw reads at that locus\n",
    "    tags_file = open(sample+\".tags.tsv\", \"r\")\n",
    "    temp_file = open(sample+\"_reads.txt\", \"w\")\n",
    "    \n",
    "    tags_file.readline()\n",
    "    for line in tags_file: \n",
    "        if not line.startswith(\"#\"):\n",
    "            if line.strip().split(\"\\t\")[2] == matchID and line.strip().split(\"\\t\")[6] == \"primary\": \n",
    "                temp_file.write(line.strip().split(\"\\t\")[9] + \"\\n\")\n",
    "    tags_file.close()\n",
    "    temp_file.close()\n",
    "    \n",
    "    #check the alleles found at that position in the raw read, using the temporary file.\n",
    "    allele1_count = 0\n",
    "    allele2_count = 0\n",
    "    other_alleles = []\n",
    "    temp_file = open(sample+\"_reads.txt\", \"r\")\n",
    "    for line in temp_file: \n",
    "        read = list(line.strip())\n",
    "        if read[snp_pos] == allele1: \n",
    "            allele1_count += 1\n",
    "        elif read[snp_pos] == allele2: \n",
    "            allele2_count += 1\n",
    "        else: \n",
    "            other_alleles.append(read[snp_pos])\n",
    "    temp_file.close()\n",
    "    \n",
    "    #delete temporary file\n",
    "    import os\n",
    "    os.remove(sample+\"_reads.txt\")\n",
    "    \n",
    "    #write out allele counts to a new file\n",
    "    newfile = open(\"SNPfinder_output.txt\", \"a\")\n",
    "    newfile.write(locus + \"\\t\"+ str(snp_pos) + \"\\t\" + sample + \"\\t\" + allele1 + \"\\t\" + str(allele1_count) +\"\\t\" + allele2 + \"\\t\" + str(allele2_count) + \"\\t\" + \",\".join(other_alleles) + \"\\n\")\n",
    "    newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TEST WITH A SMALL LIST OF INDIVIDUALS AND LOCI\n",
    "test_loci = blacklisted_loci[0:5]\n",
    "popmap = open(\"../scripts/PopMap_individs_gt50rm.txt\", \"r\")\n",
    "i = 0\n",
    "individs = []\n",
    "for line in popmap:\n",
    "    individs.append(line.strip().split()[0])\n",
    "popmap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10003  is not present in  KOD03_056\n",
      "10003  is not present in  KOD03_060\n",
      "10003  is not present in  KOD03_062\n",
      "10003  is not present in  KOD03_072\n",
      "10003  is not present in  KOD03_075\n",
      "10003  is not present in  KOD03_079\n",
      "10003  is not present in  KOD03_088\n",
      "10003  is not present in  KOD03_094\n",
      "10003  is not present in  AD06_014\n",
      "10003  is not present in  AD06_015\n",
      "10003  is not present in  AD06_024\n",
      "10003  is not present in  AD06_026\n",
      "10003  is not present in  AD06_027\n",
      "10003  is not present in  WC05_007\n",
      "10003  is not present in  WC05_011\n",
      "10003  is not present in  WC05_013\n",
      "10003  is not present in  WC05_020\n",
      "10003  is not present in  WC05_028\n",
      "10003  is not present in  WC05_042\n",
      "10003  is not present in  WC05_046\n",
      "10003  is not present in  HS04_013\n"
     ]
    }
   ],
   "source": [
    "for individ in individs: \n",
    "    SNPfinder(sample = individ, locus = \"10003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10003\t53\tKOD03_035\tG\t66\tT\t0\t\r\n",
      "10003\t53\tKOD03_051\tG\t25\tT\t0\t\r\n",
      "10003\t53\tKOD03_052\tG\t48\tT\t0\t\r\n",
      "10003\t53\tKOD03_053\tG\t13\tT\t0\t\r\n",
      "10003\t53\tKOD03_054\tG\t18\tT\t0\t\r\n",
      "10003\t53\tKOD03_055\tG\t32\tT\t0\t\r\n",
      "10003\t53\tKOD03_035\tG\t66\tT\t0\t\r\n",
      "10003\t53\tKOD03_051\tG\t25\tT\t0\t\r\n",
      "10003\t53\tKOD03_052\tG\t48\tT\t0\t\r\n",
      "10003\t53\tKOD03_053\tG\t13\tT\t0\t\r\n",
      "10003\t53\tKOD03_054\tG\t18\tT\t0\t\r\n",
      "10003\t53\tKOD03_055\tG\t32\tT\t0\t\r\n",
      "10003\t53\tKOD03_056\tG\t0\tT\t0\t\r\n",
      "10003\t53\tKOD03_057\tG\t132\tT\t0\t\r\n",
      "10003\t53\tKOD03_058\tG\t27\tT\t0\tC\r\n",
      "10003\t53\tKOD03_035\tG\t66\tT\t0\t\r\n",
      "10003\t53\tKOD03_051\tG\t25\tT\t0\t\r\n",
      "10003\t53\tKOD03_052\tG\t48\tT\t0\t\r\n",
      "10003\t53\tKOD03_053\tG\t13\tT\t0\t\r\n",
      "10003\t53\tKOD03_054\tG\t18\tT\t0\t\r\n",
      "10003\t53\tKOD03_055\tG\t32\tT\t0\t\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 50 SNPfinder_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
